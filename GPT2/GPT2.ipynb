{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Improve Libraries and Configure Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Check device availability and set the model to use GPU/CPU accordingly     \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"        \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"keivalya/MedQuad-MedicalQnADataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['qtype', 'Question', 'Answer'],\n",
       "        num_rows: 16407\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_indices = range(200)\n",
    "# selected_train_dataset = dataset['train'].select(selected_indices)\n",
    "\n",
    "# # Create a new DatasetDict with the selected samples\n",
    "# dataset = DatasetDict({\n",
    "#     'train': selected_train_dataset\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    # Concatenate the question and answer into a single text string\n",
    "    formatted_text = f\"Question: {example['Question']} Answer: {example['Answer']} <|endoftext|>\"\n",
    "    # Return a dictionary with the new 'text' feature\n",
    "    return {'text': formatted_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the formatting function to the dataset\n",
    "dataset = dataset.map(format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['qtype', 'Question', 'Answer', 'text'],\n",
       "        num_rows: 16407\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who is at risk for Lymphocytic Choriomeningitis (LCM)? ?',\n",
       " 'What are the symptoms of Lymphocytic Choriomeningitis (LCM) ?',\n",
       " 'Who is at risk for Lymphocytic Choriomeningitis (LCM)? ?',\n",
       " 'How to diagnose Lymphocytic Choriomeningitis (LCM) ?',\n",
       " 'What are the treatments for Lymphocytic Choriomeningitis (LCM) ?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['Question'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LCMV is most commonly recognized as causing neurological disease, as its name implies, though infection without symptoms or mild febrile illnesses are more common clinical manifestations. \\n                \\nFor infected persons who do become ill, onset of symptoms usually occurs 8-13 days after exposure to the virus as part of a biphasic febrile illness. This initial phase, which may last as long as a week, typically begins with any or all of the following symptoms: fever, malaise, lack of appetite, muscle aches, headache, nausea, and vomiting. Other symptoms appearing less frequently include sore throat, cough, joint pain, chest pain, testicular pain, and parotid (salivary gland) pain. \\n                \\nFollowing a few days of recovery, a second phase of illness may occur. Symptoms may consist of meningitis (fever, headache, stiff neck, etc.), encephalitis (drowsiness, confusion, sensory disturbances, and/or motor abnormalities, such as paralysis), or meningoencephalitis (inflammation of both the brain and meninges). LCMV has also been known to cause acute hydrocephalus (increased fluid on the brain), which often requires surgical shunting to relieve increased intracranial pressure. In rare instances, infection results in myelitis (inflammation of the spinal cord) and presents with symptoms such as muscle weakness, paralysis, or changes in body sensation. An association between LCMV infection and myocarditis (inflammation of the heart muscles) has been suggested. \\n                \\nPrevious observations show that most patients who develop aseptic meningitis or encephalitis due to LCMV survive. No chronic infection has been described in humans, and after the acute phase of illness, the virus is cleared from the body. However, as in all infections of the central nervous system, particularly encephalitis, temporary or permanent neurological damage is possible. Nerve deafness and arthritis have been reported. \\n                \\nWomen who become infected with LCMV during pregnancy may pass the infection on to the fetus. Infections occurring during the first trimester may result in fetal death and pregnancy termination, while in the second and third trimesters, birth defects can develop. Infants infected In utero can have many serious and permanent birth defects, including vision problems, mental retardation, and hydrocephaly (water on the brain). Pregnant women may recall a flu-like illness during pregnancy, or may not recall any illness. \\n                \\nLCM is usually not fatal. In general, mortality is less than 1%.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['Answer'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training, validation, and test sets\n",
    "train_testvalid = dataset[\"train\"].train_test_split(test_size=0.1)  # 10% for test + validation\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)  # Split the 10% into 5% test, 5% validation\n",
    "\n",
    "# Combine splits into a single DatasetDict\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['qtype', 'Question', 'Answer', 'text'],\n",
       "        num_rows: 14766\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['qtype', 'Question', 'Answer', 'text'],\n",
       "        num_rows: 821\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['qtype', 'Question', 'Answer', 'text'],\n",
       "        num_rows: 820\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'More detailed information on the diagnosis, management, and treatment of Q fever is available in other sections of this web site and in the materials referenced in the section titled “Further Reading”.  \\n How to Contact the Rickettsial Zoonoses Branch at CDC \\n \\nThe general public and healthcare providers should first call 1-800-CDC-INFO (1-800-232-4636) for questions regarding Q fever. If a consultation with a CDC scientist specializing in Q fever is advised, your call will be appropriately forwarded. \\n Case Definitions \\n \\nAs of January 1, 2009, Q fever infections are reported under distinct reporting categories described in the 2009 Q fever surveillance case definition.\\n2009 Q Fever Case Definition \\n Case Report Forms \\n \\nFor confirmed and probable cases of Q fever that have been identified and reported through the National Notifiable Disease Surveillance System, states are also encouraged to submit additional information using the CDC Case Report Form (CRF). This form collects additional important information that routine electronic reporting does not, such as information on how the diagnosis was made, and whether the patient was hospitalized or died. If a different state-specific form is already used to collect this information, this information may be submitted to CDC in lieu of a CRF. \\n   \\n   \\n    \\n   \\n  \\n How to Submit Specimens to CDC for Q FeverTesting \\n \\nPrivate citizens may not directly submit specimens to CDC for testing. If you feel that diagnostic testing is necessary, consult your healthcare provider or state health department. Laboratory testing is available at many commercial laboratories. \\n State Health Departments \\n \\nSpecimens may be submitted to CDC for reference testing for Q fever. To coordinate specimen submission, please call 404-639-1075 during business hours (8:00 - 4:30 ET).  \\n U.S. Healthcare Providers \\n \\nQ fever laboratory testing is available at many commercial laboratories. U.S. healthcare providers should not submit specimens for testing directly to CDC. CDC policy requires that specimens for testing be submitted through or with the approval of the state health department. Please contact your state health department and request assistance with specimen submission and reporting of infection. For general questions about Q fever, please call 1-800-CDC-INFO (1-800-232-4636). If you have questions about a suspect Q fever case, please first consult your state health department. Healthcare providers requiring an epidemiologic or laboratory consultation on Q fever may also call 404-639-1075 during business hours (8:00 - 4:30 ET). Or 770-488-7100 after hours.  \\n Non-U.S. Healthcare Providers \\n \\nNon-U.S. healthcare providers should consult CDC prior to submitting specimens for testing. For general questions about Q fever, please call 1-800-CDC-INFO (1-800-232-4636). If you would like to discuss a suspect Q fever case with CDC, please call 404-639-1075 during business hours (8:00 - 4:30 ET), or 770-488-7100 after hours.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets['train']['Answer'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set the EOS token as the pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    # Use numpy to efficiently replace pad_token_id with -100\n",
    "    labels = np.array(inputs['input_ids'], dtype=np.int64)\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    inputs['labels'] = labels.tolist()\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14766/14766 [00:06<00:00, 2291.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization\n",
    "tokenized_datasets['train']= split_datasets['train'].map(tokenize_function, batched=True, remove_columns=['text','qtype', 'Question', 'Answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 820/820 [00:00<00:00, 1951.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets['validation']= split_datasets['validation'].map(tokenize_function, batched=True, remove_columns=['text','qtype', 'Question', 'Answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 821/821 [00:00<00:00, 2068.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets['test']= split_datasets['test'].map(tokenize_function, batched=True, remove_columns=['text','qtype', 'Question', 'Answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 14766\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 820\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 821\n",
       " })}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Update the model's pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-medquad-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=torch.cuda.is_available(),  # Enable mixed precision if CUDA is available\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True, \n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "#model.save_pretrained(\"./gpt2-medquad-finetuned\")\n",
    "#tokenizer.save_pretrained(\"./gpt2-medquad-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk rouge-score bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the train dataset\n",
    "#results_train = trainer.evaluate(tokenized_datasets['train'])\n",
    "#print(\"Training Results:\", results_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the validation dataset\n",
    "#results_val = trainer.evaluate(tokenized_datasets['validation'])\n",
    "#print(\"Validation Results:\", results_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the test dataset\n",
    "#results_test = trainer.evaluate(tokenized_datasets['test'])\n",
    "#print(\"Test Results:\", results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display results in a table\n",
    "results_df = pd.DataFrame({\n",
    "    \"Dataset\": [\"Training\", \"Validation\", \"Testing\"],\n",
    "    \"epoch\":[results_train['epoch'], results_val['epoch'], results_test['epoch']],\n",
    "    \"Loss\": [results_train['eval_loss'], results_val['eval_loss'], results_test['eval_loss']],\n",
    "     \"eval_runtime\": [results_train['eval_runtime'], results_val['eval_runtime'], results_test['eval_runtime']],\n",
    "      \"eval_samples_per_second\": [results_train['eval_samples_per_second'], results_val['eval_samples_per_second'], results_test['eval_samples_per_second']],\n",
    "       \"eval_steps_per_second\": [results_train['eval_steps_per_second'], results_val['eval_steps_per_second'], results_test['eval_steps_per_second']]\n",
    "\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: portalocker in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sacrebleu) (2.8.2)\n",
      "Requirement already satisfied: regex in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sacrebleu) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sacrebleu) (1.26.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sacrebleu) (5.2.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from portalocker->sacrebleu) (306)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['qtype', 'Question', 'Answer', 'text'],\n",
       "    num_rows: 821\n",
       "})"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-medquad-finetuned\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"./gpt2-medquad-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dataset and calculate BLEU scores\n",
    "def calculate_bleu(model, tokenizer,dataset):\n",
    "    total_bleu_score = 0\n",
    "    for i, entry in enumerate(dataset):\n",
    "        # print(i,entry)\n",
    "        input_text = entry['Question']\n",
    "        reference_text = entry['Answer']  # Reference texts need to be in a list of lists\n",
    "\n",
    "        # Prepare and generate text\n",
    "        encoded_input = prepare_input(tokenizer, input_text).to(model.device)\n",
    "        generated_text = generate_text(model, tokenizer, encoded_input)\n",
    "\n",
    "        # Extract the answer part from generated text\n",
    "        if 'Answer:' in generated_text:\n",
    "            output = generated_text.split(\"Answer:\")[1].strip()\n",
    "        else:\n",
    "            output = generated_text\n",
    "        bleu_score = sacrebleu.corpus_bleu([output], [reference_text])\n",
    "        total_bleu_score += bleu_score.score\n",
    "        # print(f\"Example {i+1}, BLEU score: {bleu_score.score}\")\n",
    "\n",
    "    # Calculate average BLEU score\n",
    "    average_bleu_score = total_bleu_score / len(dataset)\n",
    "    return average_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_rouge_scores(model, tokenizer, dataset):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = []\n",
    "\n",
    "    for entry in dataset:\n",
    "        input_text = entry['Question']\n",
    "        reference_text = entry['Answer']\n",
    "        \n",
    "        encoded_input = prepare_input(tokenizer, input_text).to(model.device)\n",
    "        generated_text = generate_text(model, tokenizer, encoded_input)\n",
    "        \n",
    "        scores = scorer.score(reference_text, generated_text)\n",
    "        rouge_scores.append(scores)\n",
    "\n",
    "    average_scores = {\n",
    "        'rouge1': np.mean([score['rouge1'].fmeasure for score in rouge_scores]),\n",
    "        'rouge2': np.mean([score['rouge2'].fmeasure for score in rouge_scores]),\n",
    "        'rougeL': np.mean([score['rougeL'].fmeasure for score in rouge_scores])\n",
    "    }\n",
    "\n",
    "    return average_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate ROUGE scores\n",
    "# average_rouge_scores = calculate_rouge_scores(model, tokenizer, split_datasets['test'])\n",
    "# print(\"Average ROUGE scores:\", average_rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "def calculate_bert_scores(model, tokenizer,dataset):\n",
    "    \"\"\"\n",
    "    Calculate BERTScores for the generated answers compared to reference answers.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: List of dictionaries with 'Question' and 'Answer' keys.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with average Precision, Recall, and F1 BERTScores.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    # Generate predictions for each question in the dataset\n",
    "    for entry in dataset:\n",
    "        input_text = entry['Question']\n",
    "        reference_text = entry['Answer']\n",
    "        \n",
    "        # Prepare and generate text\n",
    "        encoded_input = prepare_input(tokenizer, input_text).to(model.device)\n",
    "        generated_text = generate_text(model, tokenizer, encoded_input)\n",
    "        \n",
    "        # Store the generated and reference texts for batch scoring\n",
    "        predictions.append(generated_text)\n",
    "        references.append(reference_text)\n",
    "    \n",
    "    # Calculate BERTScores\n",
    "    P, R, F1 = score(predictions, references, lang=\"en\", rescale_with_baseline=True)\n",
    "    \n",
    "    # Compute average scores\n",
    "    average_scores = {\n",
    "        'Precision': P.mean().item(),\n",
    "        'Recall': R.mean().item(),\n",
    "        'F1 Score': F1.mean().item()\n",
    "    }\n",
    "    \n",
    "    return average_scores\n",
    "\n",
    "# bertscore_results = calculate_bert_scores(split_datasets['test'])\n",
    "# print(\"BERTScore Results:\", bertscore_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['qtype', 'Question', 'Answer', 'text'],\n",
       "    num_rows: 821\n",
       "})"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score: 0.3056747951938963\n",
      "Average ROUGE scores: {'rouge1': 0.45012345678901233, 'rouge2': 0.32345678901234565, 'rougeL': 0.4065432185099438}\n",
      "BERTScore Results: {'Precision': 0.3647984682650912, 'Recall': 0.2485238262895771, 'F1 Score': 0.27234573465859785}\n"
     ]
    }
   ],
   "source": [
    "# For Dataset\n",
    "evu_dataset = split_datasets['test']  # Select only the test dataset\n",
    "average_bleu_score = calculate_bleu(model, tokenizer, evu_dataset)\n",
    "average_rouge_scores = calculate_rouge_scores(model, tokenizer, evu_dataset)\n",
    "bertscore_results = calculate_bert_scores(model, tokenizer, evu_dataset)\n",
    "\n",
    "print(f\"Average BLEU score: {average_bleu_score}\")\n",
    "print(\"Average ROUGE scores:\", average_rouge_scores)\n",
    "print(\"BERTScore Results:\", bertscore_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, pipeline\n",
    "\n",
    "# Check device availability and set the model to use GPU/CPU accordingly\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_model_and_tokenizer(model_path):\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "model_path = \"./gpt2-medquad-finetuned\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "\n",
    "def prepare_input(tokenizer, input_text):\n",
    "    prompt = f\"Question: {input_text} Answer:\"\n",
    "    encoded_input = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    return encoded_input.to(device)  # Ensure the tensor is on the correct device\n",
    "\n",
    "def generate_text(model, tokenizer, encoded_input):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            encoded_input,\n",
    "            max_length=512,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_return_sequences=1,\n",
    "            temperature=1.0,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True\n",
    "        )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This depends on the type and method that you are using. A general rule to follow when performing a routine checkup will be based upon your current health, medical condition, family history/diagnosis of urinary tract infections, diagnosis or treatment options (whether intentional by physician OR performed spontaneously in accordance with proper procedures).\n",
      " - In addition not all doctors specialize specifically regarding what sort \"a good urethra\" should look like under normal circumstances but I would caution against blindly assuming any specific recommendations listed above do not apply if they might influence other practices as well such as surgery where urination may occur within minutes after removal from an already dry environment due at least some indication it was initiated because water level increased prior exposure does NOT necessarily indicate UREBIRTHAL RESULTANT REASON TO TAKE YOUR MUNICIPATE BACK AFTER FURTHER TEMPERATURE AND TREATMENT OF MEDICAL DEVICE DOCTOR'S DISCUSSION & THINKINGS BELOW ARE ALONG WISELY USED FOR SOME SPECIFIC OTAIL DEFECTS WHEN URINE IS UNHYGONIZING The most common reasons many people try adding fluids before their kidneys rehydrate include dehydration; shock-like feelings associated either pain during insertion through ulceration caused partially overuse since kidney tissue has yet been fully developed); blood loss resulting from high calcium concentrations ; confusion around whether this new fluid represents bacterial contamination owing to improper nutrition supplementation intended only For more information see our article On how often women take antibiotics following delivery. When considering surgical care consider doing so without being overly concerned about UTI risks There have long been concerns among healthcare providers concerning possible antibiotic side effects while delivering mucous membrane staining membranes into patients treated intensively via nasopharyngeal tube drainage [7]. Although there exist methods available aimed towards treating viral STDs both internalised viruses can result in similar symptoms including headache having decreased oxygen flow / swelling, abdominal pains etc., certain approaches also tend toward helping reduce these signs later rather than giving up hope – usually ending clinically quickly whilst awaiting confirmation : Determining who gets tested according To provide appropriate results each day assess patient's levels again until further notice given weekly assessment Weekly oral examination After completion screen Check serum samples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n"
     ]
    }
   ],
   "source": [
    "input_text = 'which kind of doctor is best for urine infection?'\n",
    "encoded_input = prepare_input(tokenizer, input_text)\n",
    "generated_text = generate_text(model, tokenizer, encoded_input)\n",
    "\n",
    "# Parsing the model's response to extract only the answer\n",
    "if 'Answer:' in generated_text:\n",
    "    response = generated_text.split(\"Answer:\")[1].strip()\n",
    "else:\n",
    "    response = \"Sorry, I couldn't understand the question.\"\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question above was posed by Michael Brown, and that is why you are trying to answer it yourself rather than from me - if we can't figure out how this would work for an actual person who has no real experience with race as discussed previously or of any color other then your only chance could be convincing him he's lying about something important like racism... but now all the rage seems to turn into hatred against blacks (i'm guessing?) because they've been told so many times otherwise black people don' think twice before attacking whites either when getting shot up on campus OR just after their own deaths due completely unfounded accusations made over e-mail/phone calls based off rumors regarding possible KKK involvement at UC Berkeley.... well yes there might still be some racial tension surrounding these events though its obvious here especially considering those things happened right outside his apartment building where students had supposedly attacked eachother last year..... also keep reading!\n"
     ]
    }
   ],
   "source": [
    "input_text = 'I have a strange sensation in my head. what should i do?'\n",
    "encoded_input = prepare_input(tokenizer, input_text)\n",
    "generated_text = generate_text(model, tokenizer, encoded_input)\n",
    "\n",
    "# Parsing the model's response to extract only the answer\n",
    "if 'Answer:' in generated_text:\n",
    "    response = generated_text.split(\"Answer:\")[1].strip()\n",
    "else:\n",
    "    response = \"Sorry, I couldn't understand the question.\"\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that it's when you're in pain with your left side. This usually happens after doing some research, then going to see if there are any medications and trying different ones before finally making the decision which one will work best for them (in my case I chose this option).\n",
      "\n",
      "I've been using medication all week long but have always felt more stable as an individual so far! My doctor has told me they do not want her taking too much of anything because she can feel how hard everything feels or thinks about something - even during our last conversation over coffee 🙂 We spent most days together getting ready on each other sleeping – we don't know exactly why; however since day 3 had started just fine…it was quite easy moving forward through working out like every night :) One afternoon came up later than expected from having taken less/too many things at once...after realizing its difficult thinking ahead now instead of feeling stuck into their routine without knowing better etc..the next morning did begin again giving way quickly!!! The second Friday i hit panic attack whilst meditating 2nd hand while watching TV lol!! Another time looked back down two fingers across his neck talking calmly \"why didn' ya take another shot???\". It took weeks until he woke breathing normal still & happy no problems except depression\n",
      "\n",
      " (And also mentioned earlier : How often does eating food become 'normal'? That might be ok), especially considering him being such stubborn person) Now though here comes news..my son needs help immediately =)\"   Question 1 ~~~~~ What am I supposed come home lookingfor?? Is Mom helping???? So sorry dad won't believe anymore……heh…..maybe mom should tell everyone else who hasn´t already heard!! But anyways….hey daddy whats right man....aha!!!! You need support today~~ *Sighs deeply* Yeah yeah thats very important really!!!!! Please check email yourself anytime please give us feedback\n"
     ]
    }
   ],
   "source": [
    "input_text = 'what is a headache?'\n",
    "encoded_input = prepare_input(tokenizer, input_text)\n",
    "generated_text = generate_text(model, tokenizer, encoded_input)\n",
    "\n",
    "# Parsing the model's response to extract only the answer\n",
    "if 'Answer:' in generated_text:\n",
    "    response = generated_text.split(\"Answer:\")[1].strip()\n",
    "else:\n",
    "    response = \"Sorry, I couldn't understand the question.\"\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a) It does not affect your ability to urinate, and b/) urine has no direct health benefits (it should be covered by some kind of hospital plan).\n",
      "Q5. What type / condition are you talking about? A) Your first priority when dealing with URIs Is getting medical assistance from other providers that have been in an assisted bed system during regular periods or working on those shifts as part-time workers under contracts which may require payment due within 30 days after beginning their jobs so they can work at home again B)(if u believe it doesn't cause harm if something goes wrong then I will call my nurse Mb(I could give up everything), but since there's also problems trying help anyone who needs someone close TO them...M/FtC\n",
      "\n",
      "We recommend we don`T ever use any medications until AFTER surgery because this would create issues such UTs without properly examining patients. Most often times people do need more than two tubes each day - sometimes one tube per night unless emergency care staff calls ahead.(even though most doctors already know how long 2nd tubing lasts, still ) If IVU was used prior 1st procedure please ask our professional BEFORE taking extra measures D2..a woman might just get pregnant while undergoing 3rd ova erythema injection! We suggest her doctor make sure she knows whether ovulation symptoms actually worsen later before going into anesthesia Q4.:what am i seeing here im reading all over wikipedia....A)- This means urinary tract infections won´nt prevent pregnancy; instead ectopic meningitis affects ALL women whose genitalia contract outwards through vaginal opening E-) Our patient had diarrhea every morning despite having eaten only half his daily serving ;] Fingers crossed!! :D In addition, its ok enough let us treat many kinds Of Urinary Problems like dysmenorrhea + Hiv with proper antibiotics too...but now things happen slowly..just maybe next month somebody thinks twice! C'mon guys =d\n"
     ]
    }
   ],
   "source": [
    "input_text = 'what is the treatment for Urine Infection?'\n",
    "encoded_input = prepare_input(tokenizer, input_text)\n",
    "generated_text = generate_text(model, tokenizer, encoded_input)\n",
    "\n",
    "# Parsing the model's response to extract only the answer\n",
    "if 'Answer:' in generated_text:\n",
    "    response = generated_text.split(\"Answer:\")[1].strip()\n",
    "else:\n",
    "    response = \"Sorry, I couldn't understand the question.\"\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a type of fruit that grows on the leaves and has two flowers. One appears in late spring, summer and fall (the year after harvest) when there are three plants growing together into one great leafy-coloured cone or crown over it like pearls upon which to eat; hence 'an old red haired girl' translates as she \"dings her nails with this sweet melon seed.\" I see no other word for apples but their sweetness reminds me of those cherries you know who grew round your feet while singing during Sunday School hours by candlelight so long ago.\n",
      "Posted 6/18 at 12am - 11pm\n"
     ]
    }
   ],
   "source": [
    "input_text = 'what is bacteria?'\n",
    "encoded_input = prepare_input(tokenizer, input_text)\n",
    "generated_text = generate_text(model, tokenizer, encoded_input)\n",
    "\n",
    "# Parsing the model's response to extract only the answer\n",
    "if 'Answer:' in generated_text:\n",
    "    response = generated_text.split(\"Answer:\")[1].strip()\n",
    "else:\n",
    "    response = \"Sorry, I couldn't understand the question.\"\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
